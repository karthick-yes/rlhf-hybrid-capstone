# ==================== ENVIRONMENT ====================
env_name: 'CartPole-v1'
state_dim: 4
action_dim: 1               # Continuous Proxy: Agent sees 1 dim [-1, 1]

# Episode lengths
segment_length: 50          # Query length: Clips shown for preference comparison
max_episode_steps: 500      # Collection length: Full episode rollout limit

# ==================== DUAL-AGENT LOOP ARCHITECTURE ====================
n_rounds: 3                      # Number of exploration-exploitation cycles
n_queries_per_round: 10          # Human queries per active learning phase

# UPDATED: Fixed SAC learning issues
# - Increased SAC episodes (10→30): More high-quality data per round
# - Decreased entropy episodes (10→5): Less low-quality noise
# - Added entropy decay (0.85): Exponential reduction over rounds
# - Reduced training steps (10000→5000): Prevent overfitting on small buffer
sac_steps_per_round: 20000        # SAC training steps per round (reduced from 10000)
sac_collection_episodes: 30      # Episodes collected by SAC (increased from 10)
entropy_collection_episodes: 5   # Episodes collected by Entropy (reduced from 10)
entropy_decay: 0.85              # Decay rate per round (NEW)

# ==================== INITIAL WARMUP ====================
n_trajectories: 100         # Initial episodes (Entropy-only exploration)
n_bootstrap: 10             # Random queries to initialize ensemble

# ==================== ACTIVE LEARNING ====================
beta: 3.0                   # Revert to 3.0 to encourage auto-labeling
adaptive_beta: true         # Keep adaptive behavior
pool_size: 20             # Candidate pool size per query iteration
update_freq: 20             # Retrain ensemble every 20 queries (reduced overfitting)

# ==================== REWARD ENSEMBLE ====================
ensemble_size: 5            # Number of reward models in ensemble (reduced from 7, matches old working system)
hidden_dim: 64              # Hidden layer size for reward networks
lr: 0.003                   # Faster learning rate for CartPole (was 0.0002)
weight_decay: 0.01          # L2 regularization

# ==================== SAC HYPERPARAMETERS ====================
gamma: 0.99                 # Discount factor
tau: 0.005                  # Target network update rate
alpha: 0.05                 # Entropy regularization

# ==================== BUFFERS ====================
buffer_capacity: 100000     # Maximum trajectory storage

# ==================== SAFETY & STABILITY ====================
min_warmup_reward: 20.0     # LOWERED: Prevent infinite loops in CartPole entropy
max_defender_uncertainty: 5.0 # Widen beta if defender σ exceeds this
exploration_epsilon: 0.1    # Random query probability
defender_reset_freq: 25     # PageRank validation frequency
std_threshold: 5.0          # NEW: Higher threshold for raw reward uncertainty

# ==================== DUAL-AGENT SPECIFIC ====================
entropy_full_buffer: true   # Knowledge sharing: true = thorough exploration

# ==================== REPRODUCIBILITY ====================
seed: 42