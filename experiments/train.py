import sys
import os
import argparse
import yaml
import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm  

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.models.reward_ensemble import RewardEnsemble
from src.graph.preference_graph import PreferenceGraph
from src.acquisition.ucb_lcb import UCBLCBFilter
from src.acquisition.sampling import ActiveDethroning
from src.agents.entropy_agents import SimpleEntropySAC
from src.agents.sac import SAC
from src.envs.wrappers import make_env
from src.utils.replay_buffer import ReplayBuffer
from src.utils.buffer_adapter import SACBufferAdapter
from src.utils.logger import ExperimentLogger
from src.envs.oracle import Oracle


def load_config(config_path):
    """Load configuration from YAML file or return defaults"""
    if config_path and os.path.exists(config_path):
        print(f"Loading config from: {config_path}")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # üîß FIX: Convert string numbers to float
        numeric_keys = ['lr', 'beta', 'weight_decay', 'gamma', 'tau', 'alpha',
                        'min_warmup_reward', 'max_defender_uncertainty', 
                        'exploration_epsilon']
        
        for key in numeric_keys:
            if key in config and isinstance(config[key], str):
                try:
                    config[key] = float(config[key])
                    print(f" Converted {key} from string to float: {config[key]}")
                except ValueError:
                    print(f"Warning: Could not convert {key}={config[key]} to float")
        
        return config
    else:
        print("Using default configuration")
        return {
            'env_name': 'CartPole-v1',
            'state_dim': 4,
            'action_dim': 1,
            'beta': 3.0,
            'n_trajectories': 20,
            'n_queries': 20,
            'n_bootstrap': 5,
            'pool_size': 20,
            'update_freq': 10,
            'ensemble_size': 3,
            'hidden_dim': 256,
            'lr': 3e-4,
            'weight_decay': 1e-4,
            'sac_steps': 2000,
            'segment_length': 50,
            'buffer_capacity': 10000,
            'seed': 42
        }


class HybridTrainer:
    """
    Complete 3-phase hybrid training system.
    Integrates:
    1. PEBBLE (Entropy Warmup + Relabeling)
    2. SeqRank (Preference Graph)
    3. Active Learning (UCB/LCB + Dethroning)
    """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\n{'='*70}")
        print(f"HYBRID PREFERENCE LEARNING SYSTEM")
        print(f"{'='*70}")
        print(f"   Device: {self.device}")
        print(f"   Environment: {config.get('env_name', 'CartPole-v1')}")
        print(f"   Beta (UCB/LCB): {config.get('beta', 3.0)}")
        print(f"   Ensemble Size: {config.get('ensemble_size', 3)}")
        print(f"{'='*70}\n")
        
        # --- Environment ---
        self.env_name = config.get('env_name', 'CartPole-v1')
        self.env = make_env(
            self.env_name,
            segment_length=config.get('segment_length', 50),
            seed=config.get('seed', 42)
        )
        
        # --- Core Components ---
        self.ensemble = RewardEnsemble(
            state_dim=self.env.state_dim,
            action_dim=self.env.action_dim,
            config=config,
            device=self.device
        )
        self.graph = PreferenceGraph()
        self.ucb_lcb = UCBLCBFilter(beta=config.get('beta', 3.0))
        self.dethroning = ActiveDethroning()
        self.logger = ExperimentLogger()
        self.oracle = Oracle(self.env_name)

        # --- Data Storage ---
        self.pref_buffer = ReplayBuffer(capacity=config.get('buffer_capacity', 10000))
        self.sac_buffer = SACBufferAdapter(
            self.env.state_dim, 
            self.env.action_dim, 
            capacity=config.get('buffer_capacity', 100000),
            device=self.device
        )
        
        # --- Agents ---
        self.entropy_agent = SimpleEntropySAC(
            self.env.state_dim,
            self.env.action_dim,
            self.env.action_range,
            k=5,
            device=self.device
        )
        self.sac_agent = SAC(
            self.env.state_dim,
            self.env.action_dim,
            device=self.device,
            lr=config.get('lr', 3e-4)
        )
        
        # --- State Tracking ---
        self.defender_id = None
        self.total_human_queries = 0
        self.queried_pairs = set()  # Track (id1, id2) to avoid re-querying
        
        # --- Diagnostics ---
        self.query_log = []  # Log each query's metrics
        
        # Create Checkpoint Directory
        os.makedirs("checkpoints", exist_ok=True)

    def save_checkpoint(self, tag="latest"):
        """Save training state to disk"""
        path = os.path.join("checkpoints", f"checkpoint_{tag}.pt")
        
        # Gather ensemble state
        ensemble_state = [m.state_dict() for m in self.ensemble.models]
        optimizer_state = [opt.state_dict() for opt in self.ensemble.optimizers]
        
        checkpoint = {
            'ensemble_state': ensemble_state,
            'optimizer_state': optimizer_state,
            'sac_actor': self.sac_agent.actor.state_dict(),
            'sac_critic': self.sac_agent.critic.state_dict(),
            'total_human_queries': self.total_human_queries,
            'defender_id': self.defender_id,
            'query_log': self.query_log,
            'config': self.config
        }
        torch.save(checkpoint, path)
        # print(f"   [Checkpoint] Saved to {path}")  # Commented out to reduce clutter

    # ==================== PHASE 1: WARMUP ====================
    
    def phase1_warmup(self, n_episodes=50):
        """Phase 1: Unsupervised entropy-driven exploration."""
        print(f"\n[PHASE 1] Unsupervised Warmup (Entropy Maximization)...")
        
        trajectories, metrics = self.entropy_agent.warmup(self.env.env, n_episodes=n_episodes)
        
        count = 0
        seg_len = self.config.get('segment_length', 50)
        
        for states, actions, int_rewards, ext_rewards in trajectories:
            T = len(states)
            
            # --- Robustness Fix: Handle short trajectories ---
            if T < seg_len:
                if T >= 5:
                    true_ret = self._compute_true_reward(ext_rewards)
                    self.pref_buffer.add_trajectory(states, actions, true_ret)
                    count += 1
                continue
            # -------------------------------------------------
            
            for i in range(0, T - seg_len + 1, seg_len):
                seg_states = states[i : i+seg_len]
                seg_actions = actions[i : i+seg_len]
                true_ret = self._compute_true_reward(ext_rewards[i : i+seg_len])
                self.pref_buffer.add_trajectory(seg_states, seg_actions, true_ret)
                count += 1
                
        all_ids = self.pref_buffer.get_all_ids()
        if all_ids:
            true_rewards = [self.pref_buffer.get_trajectory(tid)['cumulative_reward'] 
                          for tid in all_ids]
            reward_mean = np.mean(true_rewards)
            reward_std = np.std(true_rewards)
            reward_max = np.max(true_rewards)
            
            print(f"   Quality check: Œº={reward_mean:.2f}, œÉ={reward_std:.2f}, max={reward_max:.2f}")
            
            min_quality_threshold = self.config.get('min_warmup_reward', 30.0)
            if reward_max < min_quality_threshold:
                print(f"Low max reward ({reward_max:.2f} < {min_quality_threshold}), extending warmup...")
                return self.phase1_warmup(n_episodes=n_episodes + 20)
        
        print(f"Phase 1 complete: {count} segments collected.\n")
        self.save_checkpoint("phase1_done")
    
    def _compute_true_reward(self, rewards):
        return float(np.sum(rewards))
    
    # ==================== BOOTSTRAP ====================
    
    def bootstrap_ensemble(self, n_bootstrap=5):
        print(f"\n[BOOTSTRAP] Making {n_bootstrap} random queries to initialize ensemble...")
        
        all_ids = self.pref_buffer.get_all_ids()
        if len(all_ids) < 2:
            print("Error: Not enough trajectories for bootstrap!")
            return
        
        for i in range(n_bootstrap):
            id1, id2 = np.random.choice(all_ids, 2, replace=False)
            r1 = self.pref_buffer.get_trajectory(id1)['cumulative_reward']
            r2 = self.pref_buffer.get_trajectory(id2)['cumulative_reward']
            label = self.oracle.compare(r1, r2)
            
            if label == 1:
                self.graph.add_preference(id1, id2, is_human_label=True)
                print(f"   Bootstrap {i+1}: {id1} > {id2}")
            else:
                self.graph.add_preference(id2, id1, is_human_label=True)
                print(f"   Bootstrap {i+1}: {id2} > {id1}")
            
            self.total_human_queries += 1
            self.queried_pairs.add((min(id1, id2), max(id1, id2)))
        
        true_rewards = {tid: self.pref_buffer.get_trajectory(tid)['cumulative_reward'] for tid in all_ids}
        best_id = max(true_rewards, key=true_rewards.get)
        self.defender_id = best_id
        print(f"   Best defender selected: {best_id} (GT reward: {true_rewards[best_id]:.2f})")
        
        _, def_std = self.compute_ensemble_stats(self.defender_id)
        max_defender_uncertainty = self.config.get('max_defender_uncertainty', 1.5)
        if def_std > max_defender_uncertainty:
            print(f"Defender uncertainty high (œÉ={def_std:.2f} > {max_defender_uncertainty})")
            print(f"   Temporarily widening Œ≤ to force more human queries...")
            self.original_beta = self.config['beta']
            self.config['beta'] = 2.0
        
        print(f"\n   Training ensemble on {n_bootstrap} bootstrap labels...")
        self.retrain_ensemble_and_relabel(verbose=False)
        
        stats = self.graph.get_stats()
        print(f"   Bootstrap complete: {stats['total']} total edges ({stats['ratio']:.2f}x augmentation)\n")
        self.validate_ensemble_calibration()
        self.save_checkpoint("bootstrap_done")
    
    # ==================== PHASE 2: ACTIVE LEARNING ====================
    
    def compute_ensemble_stats(self, traj_id):
        traj = self.pref_buffer.get_trajectory(traj_id)
        states = torch.FloatTensor(traj['states']).unsqueeze(0).to(self.device)
        actions = torch.FloatTensor(traj['actions']).unsqueeze(0).to(self.device)
        mean, std = self.ensemble.predict_segment(states, actions)
        return mean.item(), std.item()
    
    def validate_ensemble_calibration(self):
        print("\n Validating ensemble calibration...")
        all_ids = self.pref_buffer.get_all_ids()
        if len(all_ids) < 5:
            print("Warning: Not enough data for calibration check")
            return
        
        preds, truths = [], []
        for tid in all_ids:
            traj = self.pref_buffer.get_trajectory(tid)
            states = torch.FloatTensor(traj['states']).unsqueeze(0).to(self.device)
            actions = torch.FloatTensor(traj['actions']).unsqueeze(0).to(self.device)
            mean, _ = self.ensemble.predict_segment(states, actions)
            preds.append(mean.item())
            truths.append(traj['cumulative_reward'])
        
        correlation = np.corrcoef(preds, truths)[0, 1]
        print(f"   Ensemble correlation with ground truth: œÅ={correlation:.3f}")
        if correlation < 0.3:
            print(f"   Warning: Low correlation œÅ={correlation:.3f}. Continuing anyway.")
        print("   Ensemble calibration PASSED")
    
    def phase2_active_learning(self, n_queries, pool_size=50):
        print(f"\n[PHASE 2] Active Preference Learning ({n_queries} queries)...")
        
        all_ids = self.pref_buffer.get_all_ids()
        if not all_ids:
            print("Error: Buffer empty! Run Phase 1 first.")
            return
        
        self.bootstrap_ensemble(n_bootstrap=self.config.get('n_bootstrap', 5))
        
        # Add progress bar for queries
        pbar = tqdm(range(n_queries), desc="   Active Learning Loop", unit="query")
        
        for iteration in pbar:
            # Sample Candidates
            available = []
            for tid in all_ids:
                if tid == self.defender_id: continue
                pair = (min(self.defender_id, tid), max(self.defender_id, tid))
                if pair not in self.queried_pairs: available.append(tid)
            
            if len(available) == 0:
                print("   No new candidates available!")
                break
            
            pool = np.random.choice(available, size=min(pool_size, len(available)), replace=False)
            
            # Compute Stats
            def_mu, def_std = self.compute_ensemble_stats(self.defender_id)
            cand_mus = np.array([self.compute_ensemble_stats(cid)[0] for cid in pool])
            cand_stds = np.array([self.compute_ensemble_stats(cid)[1] for cid in pool])
            
            # UCB/LCB Filter
            auto_labels, uncertain_idx, dethroned, new_def = self.ucb_lcb.filter_candidates(
                def_mu, def_std, cand_mus, cand_stds, pool.tolist()
            )
            
            # Add auto-labels
            for winner, loser in auto_labels:
                w = self.defender_id if winner == 'defender' else winner
                l = self.defender_id if loser == 'defender' else loser
                self.graph.add_preference(w, l, is_human_label=False, is_auto_label=True)
            
            # Diagnostics
            auto_rate = len(auto_labels) / len(pool) if len(pool) > 0 else 0
            epsilon = self.config.get('exploration_epsilon', 0.1)
            
            if auto_rate > 0.95:
                # Silent allow
                pass
            elif np.random.random() < epsilon:
                uncertain_idx = list(range(len(pool)))
                auto_labels = []
                dethroned = False
            
            defender_changed = False
            if dethroned:
                self.defender_id = new_def
                defender_changed = True
            elif len(uncertain_idx) > 0:
                uncertain_ids = pool[uncertain_idx]
                u_mus = cand_mus[uncertain_idx]
                u_stds = cand_stds[uncertain_idx]
                
                best_idx, score, _, _ = self.dethroning.select_best_challenger(
                    def_mu, def_std, u_mus, u_stds, uncertain_ids.tolist()
                )
                challenger_id = best_idx
                
                def_r = self.pref_buffer.get_trajectory(self.defender_id)['cumulative_reward']
                chal_r = self.pref_buffer.get_trajectory(challenger_id)['cumulative_reward']
                
                label = self.oracle.compare(chal_r, def_r)
                self.total_human_queries += 1
                self.queried_pairs.add((min(self.defender_id, challenger_id), max(self.defender_id, challenger_id)))
                
                if label == 1:
                    self.graph.add_preference(challenger_id, self.defender_id, is_human_label=True)
                    self.defender_id = challenger_id
                    defender_changed = True
                else:
                    self.graph.add_preference(self.defender_id, challenger_id, is_human_label=True)
            
            # Log metrics
            stats = self.graph.get_stats()
            self.query_log.append({
                'iteration': iteration + 1,
                'total_human_queries': self.total_human_queries,
                'auto_labels': len(auto_labels),
                'auto_rate': auto_rate*100,
                'graph_edges': stats['total'],
                'augmentation': stats['ratio'],
                'defender_changed': defender_changed,
                'ensemble_std_mean': cand_stds.mean()
            })
            
            # Update Progress Bar
            pbar.set_postfix({
                'Aug': f"{stats['ratio']:.1f}x", 
                'Auto': f"{auto_rate*100:.0f}%",
                'Def': self.defender_id
            })
            
            # Restore beta
            if hasattr(self, 'original_beta'):
                self.config['beta'] = self.original_beta
                delattr(self, 'original_beta')
            
            # Periodic Updates
            update_freq = self.config.get('update_freq', 10)
            if (iteration + 1) % update_freq == 0:
                self.retrain_ensemble_and_relabel(verbose=False)
                self.save_checkpoint(f"iter_{iteration+1}")

    def retrain_ensemble_and_relabel(self, verbose=True):
        """Retrain ensemble on current graph."""
        pairs = self.graph.get_training_pairs()
        
        if len(pairs) < 5:
            if verbose: print("   Not enough data to train yet.")
            return

        epochs = 30 #reduced because it took too much time.
        samples_per_epoch = 32 
        
        # Progress bar for training
        if verbose:
            print(f"\n   {'='*60}")
            print(f"   RETRAINING ENSEMBLE")
            print(f"   {'='*60}")
            iterator = tqdm(range(epochs), desc="   Training Ensemble", unit="epoch")
        else:
            iterator = range(epochs) # Silent if inside loop
        
        for epoch in iterator:
            indices = np.random.randint(0, len(pairs), size=samples_per_epoch)
            epoch_loss = 0
            
            for i in indices:
                wid, lid = pairs[i]
                wt = self.pref_buffer.get_trajectory(wid)
                lt = self.pref_buffer.get_trajectory(lid)
                
                s0 = torch.FloatTensor(wt['states']).unsqueeze(0).to(self.device)
                a0 = torch.FloatTensor(wt['actions']).unsqueeze(0).to(self.device)
                s1 = torch.FloatTensor(lt['states']).unsqueeze(0).to(self.device)
                a1 = torch.FloatTensor(lt['actions']).unsqueeze(0).to(self.device)
                
                label_t = torch.tensor([0.0]).to(self.device)
                loss = self.ensemble.train_step(s0, a0, s1, a1, label_t)
                epoch_loss += loss
            
            avg_loss = epoch_loss / samples_per_epoch
            if verbose and isinstance(iterator, tqdm):
                iterator.set_postfix(loss=f"{avg_loss:.4f}")
        
        if verbose: print("   Ensemble Trained.")
        self.sac_buffer.relabel_and_flatten(self.pref_buffer, self.ensemble)
        if verbose: print("   SAC Buffer Relabeled.")

    # ==================== PHASE 3: POLICY LEARNING ====================

    def phase3_policy_learning(self, n_steps=5000):
        print(f"\n[PHASE 3] Policy Learning ({n_steps} steps)...")
        
        if self.sac_buffer.size < 100:
            print("Error: SAC Buffer empty.")
            return

        # Progress bar for SAC
        pbar = tqdm(range(n_steps), desc="   Training SAC Agent", unit="step")
        
        for i in pbar:
            if self.sac_buffer.size < 256: break
            actor_loss, critic_loss = self.sac_agent.update(self.sac_buffer, batch_size=256)
            
            if (i+1) % 100 == 0:
                pbar.set_postfix(act_loss=f"{actor_loss:.3f}", crit_loss=f"{critic_loss:.3f}")
        
        self.save_checkpoint("phase3_done")
        print("Phase 3 Complete.")

    # ==================== TESTING ====================
    
    def generate_toy_trajectories(self, n_trajectories=50):
        for _ in range(n_trajectories):
            states = np.random.randn(50, self.env.state_dim)
            actions = np.random.randn(50, self.env.action_dim)
            cumulative_reward = np.random.randn()
            self.pref_buffer.add_trajectory(states, actions, cumulative_reward)
    
    def run_active_loop(self, n_queries, pool_size=50):
        self.phase2_active_learning(n_queries, pool_size)

    def evaluate_final_performance(self, n_episodes=10):
        print(f"\n{'='*70}")
        print(f"FINAL EVALUATION ({n_episodes} episodes)")
        print(f"{'='*70}")
        
        random_scores = []
        for _ in range(n_episodes):
            s, _ = self.env.reset()
            score = 0
            done = False
            while not done:
                a = self.env.env.action_space.sample()
                ns, r, term, trunc, _ = self.env.step(a)
                score += r
                done = term or trunc
            random_scores.append(score)
            
        agent_scores = []
        for _ in range(n_episodes):
            s, _ = self.env.reset()
            score = 0
            done = False
            while not done:
                a = self.sac_agent.select_action(s, deterministic=True)
                ns, r, term, trunc, _ = self.env.step(a)
                score += r
                s = ns
                done = term or trunc
            agent_scores.append(score)
            
        avg_rnd = np.mean(random_scores)
        avg_agt = np.mean(agent_scores)
        
        print(f"Random Policy Average Score: {avg_rnd:.1f}")
        print(f"Hybrid Agent Average Score:  {avg_agt:.1f}")
        
        if avg_agt > avg_rnd * 2:
            print("\n‚úÖ SUCCESS: Agent is significantly better than random!")
        else:
            print("\n‚ùå FAILURE: Agent did not learn effectively.")
        print(f"{'='*70}\n")

    # ==================== ANALYSIS ====================
    
    def plot_diagnostics(self, save_path='diagnostics.png'):
        if len(self.query_log) == 0:
            print("No query log data to plot.")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
        queries = [log['iteration'] for log in self.query_log]
        
        ratios = [log['augmentation'] for log in self.query_log]
        ax1.plot(queries, ratios, linewidth=2, color='purple', marker='o')
        ax1.axhline(y=3.0, color='red', linestyle='--', label='Target: 3.0x')
        ax1.set_xlabel('Iteration Number')
        ax1.set_ylabel('Augmentation Ratio')
        ax1.set_title('Data Augmentation Over Time')
        ax1.legend()
        ax1.grid(alpha=0.3)
        
        auto_rates = [log['auto_rate'] for log in self.query_log]
        ax2.plot(queries, auto_rates, linewidth=2, color='green', marker='s')
        ax2.set_xlabel('Iteration Number')
        ax2.set_ylabel('Auto-label Rate (%)')
        ax2.set_title('UCB/LCB Filter Efficiency')
        ax2.grid(alpha=0.3)
        
        edges = [log['graph_edges'] for log in self.query_log]
        ax3.plot(queries, edges, linewidth=2, color='blue', marker='^')
        ax3.set_xlabel('Iteration Number')
        ax3.set_ylabel('Total Graph Edges')
        ax3.set_title('Preference Graph Growth')
        ax3.grid(alpha=0.3)
        
        stds = [log['ensemble_std_mean'] for log in self.query_log]
        ax4.plot(queries, stds, linewidth=2, color='orange', marker='d')
        ax4.set_xlabel('Iteration Number')
        ax4.set_ylabel('Mean Uncertainty (œÉ)')
        ax4.set_title('Ensemble Uncertainty Over Time')
        ax4.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nDiagnostics saved: {save_path}")
        plt.close()

    # ==================== MAIN ====================

    def run(self):
        self.phase1_warmup(n_episodes=self.config.get('n_trajectories', 20))
        self.phase2_active_learning(
            n_queries=self.config.get('n_queries', 20),
            pool_size=self.config.get('pool_size', 50)
        )
        self.retrain_ensemble_and_relabel(verbose=True)
        self.phase3_policy_learning(n_steps=self.config.get('sac_steps', 2000))
        
        self.graph.print_summary()
        self.plot_diagnostics()
        self.evaluate_final_performance()
        
        print("\nEXPERIMENT COMPLETE.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='RLHF Hybrid Training System')
    parser.add_argument('--config', type=str, default=None, help='Path to YAML config file')
    parser.add_argument('--beta', type=float, default=None, help='Override beta')
    parser.add_argument('--queries', type=int, default=None, help='Override number of queries')
    parser.add_argument('--bootstrap', type=int, default=None, help='Override bootstrap queries')
    args = parser.parse_args()
    
    config = load_config(args.config)
    if args.beta is not None: config['beta'] = args.beta
    if args.queries is not None: config['n_queries'] = args.queries
    if args.bootstrap is not None: config['n_bootstrap'] = args.bootstrap
    
    trainer = HybridTrainer(config)
    trainer.run()