
======================================================================
DUAL-AGENT HYBRID PREFERENCE LEARNING SYSTEM
======================================================================
   Device: cuda
   Environment: CartPole-v1
   Architecture: Iterative Dual-Agent Loop
   Beta (UCB/LCB): 7.0
   Ensemble Size: 3
   Training Rounds: 1
======================================================================


[CONFIG VERIFICATION]
   Learning Rate (lr): None
   Beta: None
   Std Threshold: None
   Hidden Dim: None
   Entropy Full Buffer: None
------------------------------

[ENV CONFIG]
   Query Length (segment_length): 50
   Collection Length (max_episode_steps): 500
   State Dim: 4
   Action Dim: 1
   Is Discrete: False

[RewardEnsemble] Using device: cuda
[RewardEnsemble] Initialized 5 models on cuda
[INITIALIZATION COMPLETE]


######################################################################
######################################################################
##  STARTING DUAL-AGENT HYBRID RLHF TRAINING
######################################################################
######################################################################


======================================================================
[PHASE 1] UNSUPERVISED WARMUP - Attempt 1
======================================================================
   Target Episodes: 10
   Using: Entropy Agent (Max-Entropy Exploration)

   Collecting (Entropy):   0%|                                                                                                  | 0/10 [00:00<?, ?ep/s]   Collecting (Entropy):  10%|#########                                                                                 | 1/10 [00:00<00:02,  3.49ep/s]   Collecting (Entropy):  60%|######################################################                                    | 6/10 [00:00<00:00, 16.69ep/s]   Collecting (Entropy): 100%|#########################################################################################| 10/10 [00:00<00:00, 22.46ep/s]                                                                                                                                                          Entropy Collection: 10 episodes, Avg Reward: 15.4, Max: 29.0, Min: 9.0, Std: 5.1

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   QUALITY CHECK
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Mean Reward: 15.40
   Max Reward:  29.00
   Std Reward:  5.08
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ“ Quality threshold met (29.00 >= -100.0)

   Phase 1 Complete: 10 trajectories collected
   All trajectories tagged as: Entropy
======================================================================

   Checkpoint saved: checkpoints\checkpoint_phase1_done.pt

======================================================================
[BOOTSTRAP] ENSEMBLE INITIALIZATION
======================================================================
   Random Queries: 5

   Bootstrap  1: 8(Entropy) > 1(Entropy) | Rewards: 13.0 > 12.0
   Bootstrap  2: 5(Entropy) > 1(Entropy) | Rewards: 29.0 > 12.0
   Bootstrap  3: 5(Entropy) > 4(Entropy) | Rewards: 29.0 > 9.0
   Bootstrap  4: 6(Entropy) > 8(Entropy) | Rewards: 17.0 > 13.0
   Bootstrap  5: 6(Entropy) > 4(Entropy) | Rewards: 17.0 > 9.0

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   REWARD DISTRIBUTION
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Min:  9.0
   Max:  29.0
   Mean: 15.4
   Std:  5.1
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

     WARNING: Max reward only 29.0 < 100
   Agent may underperform due to lack of good examples
   Consider extending warmup phase


   Training ensemble on 5 bootstrap labels...
   [Adapter] Relabeling buffer with learned rewards...
   [Adapter] Global reward stats:
      Mean: 1.97
      Std:  0.76
      Min:  0.80
      Max:  4.25
   [Adapter] Flattened into 144 transitions for SAC.
     Defender Updated: 5 (Entropy)
      - Predicted Reward: 54.5
      - True Reward:      29.0

   Bootstrap Complete:
   - Total Edges: 6
   - Augmentation: 1.20x
   - Human Queries: 5
======================================================================

   Initial Reward Correlation: 0.905

   Checkpoint saved: checkpoints\checkpoint_bootstrap_done.pt

[BASELINE EVALUATION]
Measuring random policy performance...
Random policy average: 9.6


######################################################################
##  ROUND 1 / 1
######################################################################

     Defender Updated: 5 (Entropy)
      - Predicted Reward: 54.5
      - True Reward:      29.0

======================================================================
[ACTIVE LEARNING] Round 1
======================================================================
   Queries: 5 | Pool Size: 20

   Querying:   0%|                                                                                                            | 0/5 [00:00<?, ?query/s]   Querying:   0%|                                                                              | 0/5 [00:00<?, ?query/s, Aug=1.3x, Auto=0%, Def=5(E)âœ“]   Querying:  20%|##############                                                        | 1/5 [00:00<00:01,  2.29query/s, Aug=1.3x, Auto=0%, Def=5(E)âœ“]   Querying:  20%|##############                                                        | 1/5 [00:00<00:01,  2.29query/s, Aug=1.3x, Auto=0%, Def=5(E)âœ“]   Querying:  40%|############################                                          | 2/5 [00:00<00:01,  2.68query/s, Aug=1.3x, Auto=0%, Def=5(E)âœ“]   Querying:  40%|############################                                          | 2/5 [00:01<00:01,  2.68query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying:  60%|##########################################                            | 3/5 [00:01<00:00,  2.91query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying:  60%|##########################################                            | 3/5 [00:01<00:00,  2.91query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying:  80%|########################################################              | 4/5 [00:01<00:00,  3.37query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying:  80%|########################################################              | 4/5 [00:01<00:00,  3.37query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying: 100%|######################################################################| 5/5 [00:01<00:00,  3.98query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]   Querying: 100%|######################################################################| 5/5 [00:01<00:00,  3.40query/s, Aug=1.2x, Auto=0%, Def=5(E)âœ“]

   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ROUND 1 SUMMARY
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Defender Changes: 0
   Final Defender: 5 (Entropy)
   - Predicted: 54.5
   - True:      29.0
   True Best: 5 (r=29.0)
    SUCCESS: Found the true best trajectory!
     CRITICAL: Defender NEVER changed - likely stuck!

   DUAL-AGENT BREAKDOWN:
   - SAC Wins:        0
   - Entropy Wins:    0
   - SAC Auto-Reject: 0
   - Entropy Auto-Reject: 0
   - SAC Queries:     0
   - Entropy Queries: 5
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


   ============================================================
   RETRAINING ENSEMBLE
   ============================================================
   Training:   0%|                                                                                                           | 0/30 [00:00<?, ?epoch/s]   Training:   0%|                                                                                | 0/30 [00:05<?, ?epoch/s, loss=0.0001, patience=0/3]   Training:   3%|##4                                                                     | 1/30 [00:05<02:53,  5.99s/epoch, loss=0.0001, patience=0/3]   Training:   3%|##4                                                                     | 1/30 [00:11<02:53,  5.99s/epoch, loss=0.0001, patience=1/3]   Training:   7%|####8                                                                   | 2/30 [00:11<02:32,  5.44s/epoch, loss=0.0001, patience=1/3]   Training:   7%|####8                                                                   | 2/30 [00:16<02:32,  5.44s/epoch, loss=0.0001, patience=2/3]   Training:  10%|#######2                                                                | 3/30 [00:16<02:23,  5.32s/epoch, loss=0.0001, patience=2/3]
   Early stopping at epoch 4/30 (loss=0.0001)
   Training:  10%|#######2                                                                | 3/30 [00:21<03:15,  7.23s/epoch, loss=0.0001, patience=2/3]
    Ensemble Trained.
   [Adapter] Relabeling buffer with learned rewards...
   [Adapter] Global reward stats:
      Mean: 2.04
      Std:  0.79
      Min:  0.81
      Max:  4.42
   [Adapter] Flattened into 144 transitions for SAC.
   SAC Buffer Relabeled (Size: 144)

======================================================================
[PHASE 3] POLICY LEARNING - Round 1
======================================================================
   Training Steps: 100
   SAC Buffer Size: 144

   Training SAC:   0%|                                                                                                       | 0/100 [00:00<?, ?step/s]
     Buffer exhausted at step 0
   Training SAC:   0%|                                                                                                       | 0/100 [00:00<?, ?step/s]

   Round 1 Final Performance: 9.9
======================================================================

   Checkpoint saved: checkpoints\checkpoint_round1_sac_done.pt

======================================================================
[DATA GENERATION] Round 1
======================================================================

   [1/2] SAC Collection (Exploitation)...
   SAC Collection: 2 episodes, Avg Reward: 21.0, Max: 24.0, Min: 18.0, Std: 3.0

   [2/2] Entropy Collection (Exploration)...
   > Decay applied: 2 -> 2 episodes (Rate: 0.8)
   Entropy Collection: 2 episodes, Avg Reward: 22.0, Max: 30.0, Min: 14.0, Std: 8.0

   Total Buffer Size: 14 trajectories
======================================================================

[INTERMEDIATE EVALUATION] Round 1
   Current Performance: 9.1
   vs Random Baseline:  9.6
   Improvement:         0.95x

   Checkpoint saved: checkpoints\checkpoint_round1_complete.pt

######################################################################
##  FINAL EVALUATION & DIAGNOSTICS
######################################################################


======================================================================
VALIDATING AUTO-LABEL CORRECTNESS
======================================================================
   Total preferences in graph: 12
   Correct predictions: 12
   Accuracy: 100.0%
    EXCELLENT: Auto-labels are highly accurate!
======================================================================


============================================================
PREFERENCE GRAPH SUMMARY
============================================================
  Nodes (trajectories):     9
  Direct queries (human):   10
  Auto-labels (UCB/LCB):    0
  Transitive inferences:    2
  Total edges:              12
  Augmentation ratio:       1.20x
============================================================

Top 5 trajectories:
  1. Trajectory 5: 8 wins
  2. Trajectory 6: 3 wins
  3. Trajectory 8: 1 wins
  4. Trajectory 1: 0 wins
  5. Trajectory 4: 0 wins


======================================================================
FINAL EVALUATION (20 episodes)
======================================================================

   Evaluating Random Policy...
   Random:   0%|                                                                                                                | 0/20 [00:00<?, ?it/s]                                                                                                                                                          Evaluating Trained Agent...
   Trained:   0%|                                                                                                               | 0/20 [00:00<?, ?it/s]   Trained:  85%|#####################################################################################8               | 17/20 [00:00<00:00, 168.55it/s]                                                                                                                                                       
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   PERFORMANCE COMPARISON
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Random Policy:  23.2 Â± 9.9
   Trained Agent:  9.5 Â± 0.7
   Improvement:    0.41x
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

     WARNING: Agent not significantly better than random
======================================================================


   Generating diagnostic plots...
    Diagnostics saved: diagnostics/diagnostics.png
   Generating dual-agent diagnostic plots...
   âœ“ Dual-agent stats saved: diagnostics/dual_agent_stats.png

[EXPORTING LOGS]
    Query log: logs\query_log.csv
    Dual-agent stats: logs\dual_agent_stats.csv

   round  defender_id defender_source  defender_true_reward  defender_changes  SAC_Wins  Entropy_Wins  SAC_Auto  Entropy_Auto  SAC_Query  Entropy_Query
0      1            5         Entropy                  29.0                 0         0             0         0             0          0              5

    Reward correlations: logs\reward_correlations.csv

======================================================================
[VIDEO RECORDING]
======================================================================
   Recording 3 episodes...
   Output: videos/

D:\rlhf-hybrid-capstone\.venv\lib\site-packages\gymnasium\wrappers\rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at D:\rlhf-hybrid-capstone\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
D:\rlhf-hybrid-capstone\.venv\lib\site-packages\pygame\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_stream, resource_exists
D:\rlhf-hybrid-capstone\experiments\train.py:1040: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  action = int(action > 0.5)
   Episode 1/3: 9 steps, reward=9.0
   Episode 2/3: 9 steps, reward=9.0
   Episode 3/3: 10 steps, reward=10.0

   âœ“ Videos saved to: videos/
======================================================================


######################################################################
##  EXPERIMENT COMPLETE
######################################################################
   Total Human Queries: 10
   Final Performance:   9.5
   Random Baseline:     23.2
   Improvement:         0.41x
   Graph Edges:         12
   Trajectories:        14
######################################################################

